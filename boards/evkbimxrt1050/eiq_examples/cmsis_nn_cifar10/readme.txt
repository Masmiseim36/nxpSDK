Overview
========
CMSIS-NN implementation of object detector based on the ARM ML example [1].

A 32x32 pixel color image is set as an input to a simple 3-layer convolution
neural network (CNN). Each convolution layer is followed by ReLU activation and
pooling layer. The last layer is a fully-connected layer that classifies the
input image into one of 10 output classes: "airplane", "automobile", "bird",
"cat", "deer", "dog", "frog", "horse", "ship" and "truck". The CNN used in this
example is based on the CIFAR-10 example from Caffe [2]. 

The example model implementation needs 87 KB to store weights,
40 KB for activations and 6 KB for storing the im2col data.

Firstly a static ship image is used as input regardless of camera is connected
or not. Secondly runtime image processing from camera in the case camera and
display are connected. Camera data are displayed on LCD.

HOW TO USE THE APPLICATION:
To classify an image, place an image in front of the camera so that it fits in
the white rectangle in the middle of the display. 
Note semihosting implementation causes slower or discontinuous video experience. 
Select UART in 'Project Options' during project import for using external debug
console via UART (virtual COM port).

[1] https://github.com/ARM-software/ML-examples/tree/master/cmsisnn-cifar10
[2] https://github.com/BVLC/caffe

Files:
  cifar10.c - example source code
  ship.bmp - shrinked picture of the object to recognize
    (source: https://en.wikipedia.org/wiki/File:Christian_Radich_aft_foto_Ulrich_Grun.jpg)
  ship.h - image file converted into a C language array of RGB values
    using Python with the OpenCV and Numpy packages:
    import cv2
    import numpy as np
    img = cv2.imread('ship.bmp')
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    with open('ship_image.h', 'w') as fout:
      print('#define SHIP_IMG_DATA {', file=fout)
      img.tofile(fout, ',', '%d')
      print('}\n', file=fout)
  weights.h - neural network weights and biases generated by scripts available at [1]
  parameter.h - parameters of the neural network generated by scripts available at [1]
  timer.c - timer source code
  image/* - image capture and pre-processing code
  video/* - camera and display handling


Toolchain supported
===================
- IAR embedded Workbench  8.50.9
- Keil MDK  5.33
- GCC ARM Embedded  9.3.1
- MCUXpresso  11.3.0

Hardware requirements
=====================
- Mini/micro USB cable
- EVKB-IMXRT1050 board
- Personal computer
- MT9M114 camera (optional)
- RK043FN02H-CT display (optional)

Board settings
==============
Connect the camera to J35 (optional)
Connect the display to A1-A40 and B1-B6 (optional)
Connect external 5V power supply to J2, set J1 to 1-2

Prepare the Demo
================
1. Connect a USB cable between the host PC and the OpenSDA USB port on the target board. 
2. Open a serial terminal with the following settings:
   - 115200 baud rate
   - 8 data bits
   - No parity
   - One stop bit
   - No flow control
3. Download the program to the target board.
4. Either press the reset button on your board or launch the debugger in your IDE to begin running the demo.

Running the demo
================
The log below shows the output of the demo in the terminal window (compiled with ARM GCC):

CIFAR-10 object recognition example using CMSIS-NN.
Detection threshold: 60%

Static data processing:
----------------------------------------
     Inference time : 55 ms    
     Detected: ship (99%)
----------------------------------------


Camera data processing:
Data for inference are ready
----------------------------------------
     Inference time : 56 ms    
     Detected: bird (93%)
----------------------------------------
 
Data for inference are ready
----------------------------------------
     Inference time : 54 ms    
     Detected: airplane (99%)
----------------------------------------

Data for inference are ready
----------------------------------------
     Inference time : 54 ms    
     Detected: horse (99%)
----------------------------------------
