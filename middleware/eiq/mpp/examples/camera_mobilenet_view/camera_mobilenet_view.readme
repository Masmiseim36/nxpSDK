Overview
========

This example shows how to use the library to create a use-case for
image classification using camera as source.

The machine learning frameworks used are TensorFlow Lite Micro, GLOW or DeepViewRT
The image classification model used is quantized Mobilenet
convolutional neural network model [1] that classifies the input image into
one of 1000 output classes.

[1] https://www.tensorflow.org/lite/models


Use-cases Description
=====================

HOW TO USE THE APPLICATION:
To classify an image, place an image in front of camera so that it fits in the blue rectangle in the middle of the display.
Some images will work better than others.

1-a) High-level description
     ----------------------

                                                                   +------------------------------------------+
                                                                   |                                          |
                                                                   |                                          |
                                                                  \ /                                         |
                  +-------------+      +-------------+      +-------------+      +-------------+              |
                  |             |      |             |      |             |      |             |              |
Pipeline 0        |    camera   | -->  |  2D convert | -->  |   labeled   | -->  |    Display  |              |
                  |             |  |   |             |      |  rectangle  |      |             |              |
                  +-------------+  |   +-------------+      +-------------+      +-------------+              |
                                   |                                                                          |
                                   |     +-------------+      +--------------+      +-------------+           |
                                   |     |             |      |              |      |             |           |
Pipeline 1                         +---> |  2D convert | -->  | ML Inference | -->  |  NULL sink  |           |
                                         |             |      |              |      |             |           |
                                         +-------------+      +--------------+      +-------------+           |
                                                                       |                                      |
                                                                       |                                      |
        +-----------------+                                            |                                      |
	|  Main app:      |                                            |                                      |
	| ML output       |   <----- ML Inference output callback -----+                                      |
        | post processing |                                                                                   |
	|                 |   ------ labeled rectangle update ------------------------------------------------+
	+-----------------+


1-b) Detailed description
     --------------------

Application creates two pipelines:

- One pipeline that runs the camera preview.
- Another pipeline that runs the ML inference on the image coming from the camera.
- Pipeline 1 is split from pipeline 0
- Pipeline 0 executes the processing of each element sequentially and CANNOT be preempted by another pipeline.
- Pipeline 1 executes the processing of each element sequentially but CAN be preempted.

1-c) Pipelines elements description
     ------------------------------

* Camera element is configured for a specific pixel format and resolution (board dependent)
* Display element is configured for a specific pixel format and resolution (board dependent)
* 2D convert element on pipeline 0 is configured to perform:
  - color space conversion from the camera pixel format to the display pixel format
  - rotation depending on the camera vs display orientation

* 2D convert element on pipeline 1 is configured to perform:
  - color space conversion from the camera pixel format to RGB888
  - cropping to maintain image aspect ratio
  - scaling to 128x128 as mandated by the image classification model

* The labeled rectangle element draws a crop window from which the camera image is sent to
  the ML inference element. The labeled rectangle element also displays the label of the object detected.
* The ML inference element runs an inference on the image pre-processed by the 2D convert element.
* The NULL sink element closes pipeline 1 (in MPP concept, only sink elements can close a pipeline).

* At every inference, the ML inference element invokes a callback containing the inference outputs.
These outputs are post-processed by the callback client component (in this case, the main task of the application)


Running the demo
================

EXPECTED OUTPUTS:
The expected outputs of the example are:
- Detected label should be displayed on the screen
- Pipeline tasks statistics are displayed on the console
- Logs below should be displayed on the debug console

Logs for camera_mobilenet_view example using TensorFlow Lite Micro model should look like this:

[MPP_VERSION_1.0.0]
Inference Engine: TensorFlow-Lite Micro 
----------------------------------------
     Inference time: 47 ms 
     Detected: spider web (35%)
----------------------------------------
API stats ------------------------------
rc_cycle = 20 ms rc_cycle_max 99 ms
pr_slot  = 79 ms pr_rounds 1 app_slot 27 ms
MPP stats ------------------------------
mpp 800831D8 exec_time 20 ms
mpp 800834B8 exec_time 52 ms
----------------------------------------
     Inference time: 47 ms 
     Detected: No label detected (0%)
----------------------------------------
API stats ------------------------------
rc_cycle = 20 ms rc_cycle_max 99 ms
pr_slot  = 79 ms pr_rounds 1 app_slot 26 ms
MPP stats ------------------------------
mpp 800831D8 exec_time 20 ms
mpp 800834B8 exec_time 52 ms

Important notes
===============

This project embeds TensorFlow Lite Micro framework by default.
TensorFLow Lite Micro framework is an optional component of MPP.
It has some dependencies on the operations that must be built to support a specific TensorFlow Lite model.
This dependency must be resolved by the definition of a function called:

tflite::MicroOpResolver &MODEL_GetOpsResolver(tflite::ErrorReporter* errorReporter)

This example has a default Inference Engine, but it may use another:
- Glow or DeepViewRT inference by setting the flag TFLITE_CUSTOM_OPS_DUMMY_RESOLVER.
- TensorFlow Lite Micro by removing the flag TFLITE_CUSTOM_OPS_DUMMY_RESOLVER.
  Then the user may either set the flag TFLITE_ALL_OPS_RESOLVER,
  or implement its own function MODEL_GetOpsResolver for smaller memory footprint.
