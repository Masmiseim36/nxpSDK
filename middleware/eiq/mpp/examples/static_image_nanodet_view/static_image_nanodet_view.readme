Overview
========

This example shows how to use the library to create one object detection using a file as source

The machine learning frameworks used are TensorFlow Lite Micro or GLOW
The object detection model used is quantized Nanodet M model that detects multiple objects in an input image.
The model has 80 classes.


Use-case Description
=====================

1-a) High-level description
     ----------------------

                                                                   +------------------------------------------+
                                                                   |                                          |
                                                                   |                                          |
                                                                  \ /                                         |
                  +-------------+      +-------------+      +-------------+      +-------------+              |
                  |    static   |      |             |      |             |      |             |              |
Pipeline 0        |     image   | -->  |  2D convert | -->  |   labeled   | -->  |    Display  |              |
                  |             |  |   |             |      |  rectangle  |      |             |              |
                  +-------------+  |   +-------------+      +-------------+      +-------------+              |
                                   |                                                                          |
                                   |     +-------------+      +--------------+      +-------------+           |
                                   |     |             |      |              |      |             |           |
Pipeline 1                         +---> |  2D convert | -->  | ML Inference | -->  |  NULL sink  |           |
                                         |             |      |              |      |             |           |
                                         +-------------+      +--------------+      +-------------+           |
                                                                       |                                      |
                                                                       |                                      |
    +-----------------+                                                |                                      |
	|  Main app:      |                                                |                                      |
	| ML output       |   <----- ML Inference output callback ---------+                                      |
    | post processing |                                                                                       |
	|                 |   ------ Adding detected labeled rectangles ------------------------------------------+
	+-----------------+


1-b) Detailed description
     --------------------

Application creates two pipelines:

- One pipeline that runs the static image preview.
- Another pipeline that runs the ML inference on the static image.
- Pipeline 1 is split from pipeline 0
- Pipeline 0 executes the processing of each element sequentially and CANNOT be preempted by another pipeline.
- Pipeline 1 executes the processing of each element sequentially but CAN be preempted.

1-c) Pipelines elements description
     ------------------------------

* Static image 320x320 BGRA format
* Display element is configured for a specific pixel format and resolution (board dependent)
* 2D convert element on pipeline 0 is configured to perform:
  - color space conversion from ARGB to the display pixel format
  - Scaling from 320x320 to the display resolution

* 2D convert element on pipeline 1 is configured to perform:
  - color space conversion from ARGB to RGB888
  - cropping to maintain image aspect ratio
  - scaling to 128x128 as mandated by the object detection model

* The labeled rectangle element draws a crop window from which the static image is sent to
  the ML inference element. The labeled rectangle element also displays the labels and boxes of the detected objects.
* The ML inference element runs an inference on the image pre-processed by the 2D convert element.
* The NULL sink element closes pipeline 1 (in MPP concept, only sink elements can close a pipeline).

* At every inference, the ML inference element invokes a callback containing the inference outputs.
These outputs are post-processed by the callback client component(in this case, the main task of the application) 
that determinates the boxes of detected objects and performs NMS to pick the best box for each detected object.

Running the demo
================

EXPECTED OUTPUTS:
The expected outputs of the example are:
- For each detected object, a labeled rectangle should be displayed on the screen
- Logs below should be displayed on the debug console

Logs for static_image_nanodet_m_view example using TensorFlow Lite Micro model should look like this:

[MPP_VERSION_1.0.0]
Inference Engine: TensorFlow-Lite Micro 
Element stats --------------------------
nanodet : exec_time 615 ms
nanodet : box 1 label person score 80(%)
Element stats --------------------------
nanodet : exec_time 615 ms
nanodet : box 1 label person score 80(%)
Element stats --------------------------
nanodet : exec_time 615 ms
nanodet : box 1 label person score 80(%)
Element stats --------------------------
nanodet : exec_time 617 ms
nanodet : box 1 label person score 80(%)

Important notes
===============

This project embeds TensorFlow Lite Micro framework by default.
TensorFLow Lite Micro framework is an optional component of MPP.
It has some dependencies on the operations that must be built to support a specific TensorFlow Lite model.
This dependency must be resolved by the definition of a function called:

tflite::MicroOpResolver &MODEL_GetOpsResolver(tflite::ErrorReporter* errorReporter)

This example uses TensorFlow Lite Micro, but it may use another inference engine:
- Glow or DeepViewRT inference by setting the flag TFLITE_CUSTOM_OPS_DUMMY_RESOLVER.
- TensorFlow Lite Micro by removing the flag TFLITE_CUSTOM_OPS_DUMMY_RESOLVER.
  Then the user may either set the flag TFLITE_ALL_OPS_RESOLVER,
  or implement its own function MODEL_GetOpsResolver for smaller memory footprint.
