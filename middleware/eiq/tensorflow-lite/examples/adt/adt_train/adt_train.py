"""
   Copyright 2017 The TensorFlow Authors. All Rights Reserved.
   Copyright 2019 NXP. All Rights Reserved.
   
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"""

import pandas as pd
import numpy as np

import sys
import argparse

from sklearn.model_selection import train_test_split
from keras.models import Model, load_model
from keras.layers import Input, Dense
from keras import regularizers
from keras.initializers import RandomUniform
from keras.optimizers import SGD
import tensorflow as tf
from keras import backend as K
FLAGS = None

def preprocess_data(data):
    #data = (FLAGS.scale_range/3*(data - data.mean())/data.std()).clip(-1,1)
    data = data.diff()[5:]
    data[['ax','ay','az']] /= 10
    #data = (data + 1)/2
    return data


def main(_):
    chanels = ['wx', 'wy', 'wz', 'ax', 'ay', 'az']
    patch_size = 5
   
    # load input data
    data = pd.read_csv(FLAGS.input_data,
                       delimiter = ',', dtype = np.int, header=0)
    data = preprocess_data(data)
    
    # prepare patches   
    samples_nb = int(np.floor(data.shape[0] / float(patch_size))) * patch_size
    patch = np.reshape(data[chanels][:samples_nb].values,
                       (-1,len(chanels)*patch_size))
                   
    # split into train and test set
    train_set, test_set = train_test_split(patch, test_size=0.1, random_state=42)

    if FLAGS.train_keras_model:
        # create autoencoder model
        input_dim = test_set.shape[1]   
        encoding_dim = 16 
    
        
        input_layer = Input(shape=(input_dim, ))
        encoder = Dense(encoding_dim, activation='tanh', 
                        activity_regularizer=regularizers.l1(10e-7))(input_layer)
        encoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)
        
        decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)
        decoder = Dense(input_dim, activation='tanh')(decoder)
        autoencoder = Model(inputs=input_layer, outputs=decoder)
        
        tf.contrib.quantize.create_training_graph(input_graph=K.get_session().graph, quant_delay=int(0))
        
        autoencoder.compile(optimizer='adamax',
                            loss='mean_squared_error', 
                            metrics=['accuracy'])
        
        # set training parameters
        nb_epoch = 1000
        batch_size = 10
    
        # train model
        history = autoencoder.fit(train_set,train_set,
                                  epochs=nb_epoch,
                                  batch_size=batch_size,
                                  shuffle=True,
                                  validation_data=(test_set,test_set),
                                  verbose=1)
        threshold = 3 * history.history['val_loss'][-1]    
        autoencoder._make_predict_function()
        
        # save keras model
        autoencoder.save(FLAGS.keras_model)    
    else:
      # Recreate the exact same model, including weights and optimizer.
      autoencoder = load_model(FLAGS.keras_model)
      autoencoder._make_predict_function()
      
      predictions = autoencoder.predict(test_set)
      error = np.mean(np.square(predictions - test_set), 1)
      threshold = 3 * np.mean(error)
      
      
    # convert keras model into tf lite model and store it
    converter = tf.lite.TFLiteConverter.from_keras_model_file(FLAGS.keras_model)
    tflite_model = converter.convert()
    open(FLAGS.tflite_model, "wb").write(tflite_model)
    
    # convert to C source code                  
    with open(FLAGS.tflite_model, 'rb') as f_in:
        x = [hex(b) for b in f_in.read()]
        with open(FLAGS.c_model, 'w') as f_out:
            f_out.write(("// This file is generated by adt_train.py\n\n" +
                         "const char adt_model[] = {{{}}};\n" +
                         "unsigned int adt_model_len = {};")
                        .format(", ".join(x), len(x)))
    
    # convert keras model into quantized tf lite model and store it
    converter = tf.lite.TFLiteConverter.from_keras_model_file(FLAGS.keras_model)
    converter.inference_type = tf.uint8
    converter.default_ranges_stats = (-1,1)
    converter.quantized_input_stats = {'input_1':(0,0.2)}
    tflite_model = converter.convert()
    open(FLAGS.tflite_quantized_model, "wb").write(tflite_model)
    
    # convert to C source code                  
    with open(FLAGS.tflite_quantized_model, 'rb') as f_in:
        x = [hex(b) for b in f_in.read()]
        with open(FLAGS.c_quantized_model, 'w') as f_out:
            f_out.write(("// This file is generated by adt_train.py\n\n" +
                         "const char adt_model[] = {{{}}};\n" +
                         "unsigned int adt_model_len = {};")
                        .format(", ".join(x), len(x)))
        
    if len(FLAGS.validation_data) > 0:
        data = pd.read_csv(FLAGS.validation_data,
                           delimiter = ',', dtype = np.int, header=0)
        
        if len(FLAGS.validation_data_c) > 0:
            c_source_contain = ("// This file is generated by adt_train.py\n\n" +
                                '#ifndef COMMANDS_H_\n' +            
                                '#define COMMANDS_H_\n\n'+
                                'const int16_t in_data[] = {{\n {}}};\n\n' +
                                '#endif /* COMMANDS_H_ */')
                                 
            with open(FLAGS.validation_data_c, 'w') as f:
                f.write(c_source_contain.format(np.array2string(data[chanels].values.flatten(),
                                                                threshold = data[chanels].size,
                                                                separator=',')[1:-1]))
                                
        
        # normalize data
        data = preprocess_data(data)
        
        # prepare patches
        samples_nb = int(np.floor(data.shape[0] / float(patch_size))) * patch_size
        patch = np.reshape(data[chanels][:samples_nb].values,
                           (-1,len(chanels)*patch_size))
        
        # evaluate
        predictions = autoencoder.predict(patch)
        error = np.mean(np.square(predictions - patch), 1)
        
        for i, err in enumerate(error):
            print(i, err)            
        
        with open('parameters.h', 'w') as f:
            f.write('// This file is generated by adt_train.py\n\n')
            f.write('const int PATCH_SIZE = %d;\n'%(patch_size))
            f.write('const int  NUM_CHANELS = %d;\n'%(len(chanels)))
            f.write('const float THRESHOLD = %f;\n'%(threshold))
            
        if (FLAGS.vizualize):
            import matplotlib.pyplot as plt
            
            x = np.arange(predictions.shape[0])
            x_dense = np.arange(0,predictions.shape[0],0.02)
            y = np.interp(x_dense, x, error)
            y_a = np.ma.masked_less(y, threshold)
            
            fig, ax = plt.subplots()
            ax.plot(x_dense, y, '-k')
            ax.plot(x_dense, y_a, '-r')
            ax.set_xticks(np.arange(0,201,10))
            ax.grid(which='major', axis='x', linestyle='--')
            
            

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
      '--input_data',
      type=str,
      # pylint: disable=line-too-long
      default='input_data.txt',
      # pylint: enable=line-too-long
      help='Location of input training data.')
    
    parser.add_argument(
      '--keras_model',
      type=str,
      # pylint: disable=line-too-long
      default='keras_model.h5',
      # pylint: enable=line-too-long
      help='Location where output keras model will be stored.')    
    
    parser.add_argument(
      '--train_keras_model',
      type=bool,
      # pylint: disable=line-too-long
      default=True,
      # pylint: enable=line-too-long
      help='If True than train a new model else use an existing model.')
    
    parser.add_argument(
      '--tflite_model',
      type=str,
      # pylint: disable=line-too-long
      default='adt_model.tflite',
      # pylint: enable=line-too-long
      help='Location where output tflite model will be stored.')
    
    parser.add_argument(
      '--tflite_quantized_model',
      type=str,
      # pylint: disable=line-too-long
      default='adt_model_quant.tflite',
      # pylint: enable=line-too-long
      help='Location where output quantized tflite model will be stored.')
    
    parser.add_argument(
      '--c_quantized_model',
      type=str,
      # pylint: disable=line-too-long
      default='adt_model_quant.h',
      # pylint: enable=line-too-long
      help='Location where quantized model as c source code will be stored.')
    
    parser.add_argument(
      '--c_model',
      type=str,
      # pylint: disable=line-too-long
      default='adt_model.h',
      # pylint: enable=line-too-long
      help='Location where model as c source code will be stored.')
      
    parser.add_argument(
      '--validation_data',
      type=str,
      # pylint: disable=line-too-long
      default='',
      # pylint: enable=line-too-long
      help='If specified, evaluate validation data.')
    
    parser.add_argument(
      '--validation_data_c',
      type=str,
      # pylint: disable=line-too-long
      default='commands.h',
      # pylint: enable=line-too-long
      help='If specified, store validation data as c variable.')
    
    parser.add_argument(
      '--vizualize',
      type=bool,
      # pylint: disable=line-too-long
      default=False,
      # pylint: enable=line-too-long
      help='Show evaluation data.')
      
    FLAGS, unparsed = parser.parse_known_args()
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)